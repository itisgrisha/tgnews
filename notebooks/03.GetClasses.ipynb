{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import artm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain \n",
    "from collections import Counter\n",
    "from ufal.udpipe import Model, Sentence, ProcessingError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"/eee/tgnews/meta/\"\n",
    "\n",
    "columns = [\"path\", \"og:site_name\", \"og:url\"]\n",
    "\n",
    "table_ru = pd.read_csv(os.path.join(load_path, \"ru.tsv\"), sep='\\t', usecols=columns, keep_default_na=False, quoting=csv.QUOTE_NONE)\n",
    "table_en = pd.read_csv(os.path.join(load_path, \"en.tsv\"), sep='\\t', usecols=columns, keep_default_na=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266102, 233388)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table_ru), len(table_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выделение токенов из url\n",
    "\n",
    "def url_to_tokens(url, lowercase=True, split_pattern=\"[^a-z]+\", filter_pattern=\"[a-z]{3,}\"):\n",
    "    \n",
    "    tokens = re.sub(\"https*://[^/]*\", \"\", url).strip(\"/\").split(\"/\")[:-1]\n",
    "    \n",
    "    if lowercase:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        \n",
    "    if split_pattern is not None:\n",
    "        tokens = list(chain(*[re.split(split_pattern, token) for token in tokens]))\n",
    "        \n",
    "    if filter_pattern is not None:\n",
    "        tokens = list(filter(lambda x: re.search(filter_pattern, x), tokens))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "table_ru[\"tokens:url\"] = table_ru[\"og:url\"].apply(url_to_tokens)\n",
    "table_en[\"tokens:url\"] = table_en[\"og:url\"].apply(url_to_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Формируем список наиболее частотных токенов\n",
    "\n",
    "site_tokens = dict([(site, Counter(tokens)) for site, tokens in table_ru.groupby(\"og:site_name\")[\"tokens:url\"].sum().iteritems()])\n",
    "\n",
    "for site, tokens in table_en.groupby(\"og:site_name\")[\"tokens:url\"].sum().iteritems():\n",
    "    \n",
    "    site_tokens.setdefault(site, Counter())\n",
    "    site_tokens[site] += Counter(tokens)\n",
    "    \n",
    "token_count = {}\n",
    "token_sites = {}\n",
    "\n",
    "for site, tokens in site_tokens.items():\n",
    "    for token, count in tokens.items():\n",
    "        \n",
    "        token_count.setdefault(token, 0)\n",
    "        token_count[token] += count\n",
    "        \n",
    "        token_sites.setdefault(token, [])\n",
    "        token_sites[token].append(site)\n",
    "        \n",
    "tokens = [token for token, count in token_count.items() if (count >= 100) and (len(token_sites[token]) >= 10)]\n",
    "\n",
    "with open(\"../url_tokens.txt\", \"w\") as fl:\n",
    "    fl.write('\\n'.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class_news_by_url(url_tokens):\n",
    "    \n",
    "    url_tokens = \" \".join(url_tokens)\n",
    "    classes = []\n",
    "    \n",
    "    for part in [\"society\",\n",
    "                 \"politic\",\n",
    "                 \"incident\",\n",
    "                 \"criminal\",\n",
    "                 \"proisshestviya\",\n",
    "                 \"obshchestvo\",\n",
    "                 \"social\",\n",
    "                 \"murder\",\n",
    "                 \"politika\",\n",
    "                 \"crime\",\n",
    "                 \"kriminal\",\n",
    "                 \"parliament\",\n",
    "                 \"president\",\n",
    "                 \"law\",\n",
    "                 \"government\",\n",
    "                 \"political\"]:\n",
    "    \n",
    "        if part in url_tokens:\n",
    "            classes.append(\"Society\")\n",
    "            break\n",
    "        \n",
    "    for part in [\"econom\",\n",
    "                 \"ekonomika\",\n",
    "                 \"business\",\n",
    "                 \"industry\",\n",
    "                 \"biznes\",\n",
    "                 \"bank\",\n",
    "                 \"energy\",\n",
    "                 \"company\",\n",
    "                 \"finance\"]:\n",
    "    \n",
    "        if part in url_tokens:\n",
    "            classes.append(\"Economy\")\n",
    "            break\n",
    "        \n",
    "    for part in [\"sport\",\n",
    "                 \"hockey\",\n",
    "                 \"football\",\n",
    "                 \"tennis\",\n",
    "                 \"basketball\",\n",
    "                 \"box\",\n",
    "                 \"fitness\",\n",
    "                 \"athletic\",\n",
    "                 \"golf\"]:\n",
    "        \n",
    "        if part in url_tokens:\n",
    "            classes.append(\"Sports\")\n",
    "            break\n",
    "        \n",
    "    for part in [\"auto\",\n",
    "                 \"tehn\",\n",
    "                 \"internet\",\n",
    "                 \"digital\",\n",
    "                 \"mobile\",\n",
    "                 \"gadgets\",\n",
    "                 \"smart\"]:\n",
    "        \n",
    "        if part in url_tokens:\n",
    "            classes.append(\"Technology\")\n",
    "            break\n",
    "        \n",
    "    for part in [\"art\",\n",
    "                 \"kino\",\n",
    "                 \"culture\",\n",
    "                 \"kultura\",\n",
    "                 \"entertainment\",\n",
    "                 \"afisha\",\n",
    "                 \"cinema\",\n",
    "                 \"book\",\n",
    "                 \"game\",\n",
    "                 \"music\",\n",
    "                 \"party\",\n",
    "                 \"gaming\"]:\n",
    "        \n",
    "        if part in url_tokens:\n",
    "            classes.append(\"Entertainment\")\n",
    "            break\n",
    "    \n",
    "    for part in [\"nauka\",\n",
    "                 \"health\",\n",
    "                 \"research\",\n",
    "                 \"science\",\n",
    "                 \"medicine\"]:\n",
    "        \n",
    "        if part in url_tokens:\n",
    "            classes.append(\"Science\")\n",
    "            break\n",
    "    \n",
    "    if not classes:\n",
    "        return None\n",
    "    \n",
    "    return classes\n",
    "\n",
    "table_ru[\"class\"] = table_ru[\"tokens:url\"].apply(predict_class_news_by_url)\n",
    "table_en[\"class\"] = table_en[\"tokens:url\"].apply(predict_class_news_by_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_token = \"features\"\n",
    "\n",
    "paths = table_ru[table_ru[\"tokens:url\"].apply(lambda x: url_token in x)][\"path\"].tolist()\n",
    "paths = paths + table_en[table_en[\"tokens:url\"].apply(lambda x: url_token in x)][\"path\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\"/>\n",
      "    <meta property=\"og:url\" content=\"https://nowtoronto.com/movies/features/disney-frozen-2-indigenous-culture-sami/\"/>\n",
      "    <meta property=\"og:site_name\" content=\"NOW\"/>\n",
      "    <meta property=\"article:published_time\" content=\"2019-11-19T16:32:33+00:00\"/>\n",
      "    <meta property=\"og:title\" content=\"Disney signed a contract with Indigenous people before making Frozen II\"/>\n",
      "    <meta property=\"og:description\" content=\"After a debate about cultural appropriation surrounded Frozen, Disney agreed in writing to respectfully portray Sámi culture in the sequel\"/>\n",
      "  </head>\n",
      "  <body>\n",
      "    <article>\n",
      "      <h1>Disney signed a contract with Indigenous people before making Frozen II</h1>\n",
      "      <h2>After a debate about cultural appropriation surrounded Frozen, Disney agreed in writing to respectfully portray Sámi culture in the sequel</h2>\n",
      "      <address><time datetime=\"2019-11-19T16:32:33+00:00\">19 Nov 2019, 16:32</time> by <a rel=\"author\">Radheyan Simonpillai</a></address>\n",
      "      <p>The animated Disney film Frozen opened with a tribal-sounding choral chant that, for many in North America, sounded completely out of place.</p>\n",
      "      <p>The 2013 movie was about blond-haired, blue-eyed sisters Elsa and Anna, who reign over the Norwegian kingdom of Arendelle. Elsa’s show-stopping Broadway-worthy anthem Let It Go was a far cry from that puzzling opening chant.</p>\n",
      "      <p>However, Sámi people – the Indigenous communities in Scandinavian regions – recognized the tune right away. It turns out the context behind that music was subject to erasure.</p>\n",
      "      <p>The song is <a href=\"https://youtu.be/schn6IIJd-U\">Vuelie</a> and it was written for the film by South Sámi musician and composer Frode Fjellheim, who adapted it from one of his earlier songs, Eatnemen Vuelie (Song Of The Earth). His music draws on joik, an ancient vocal tradition that was outlawed when Nordic Indigenous communities were Christianized (like what Canada did with the Potlatch ban).</p>\n",
      "      <p>The selective use of Sámi culture in Frozen led to debate on social media about appropriation and whitewashing, and not just because of the joiking. The character Kristoff (voiced by Jonathan Groff) has a wardrobe that resembles what Sámi reindeer herders would wear, but he looks very Norwegian. To be fair, Sámi people can have blond hair and blue eyes – the result of forced assimilation and ethnic cleansing for over a century.</p>\n",
      "      <p>To make sure cultural erasure didn't happen in Frozen II, Sámi leaders entered into a contract with Disney that affirms ownership of their culture.</p>\n",
      "      <p>This time, filmmakers Jennifer Lee, Chris Buck and producer Peter Del Vecho sought out expert advice on how to respectfully portray Indigenous culture, which is heavily and intricately featured in the film and its reconciliation plot.</p>\n",
      "      <p>The sequel finds Elsa summoned by a mysterious siren, a melodic voice calling from a far off land that holds buried secrets about Arendelle’s past. The ensuing story enriches and expands the mythology behind its characters. It turns out Elsa and a fictional community called Northuldra (who are inspired by the Sámi) have a shared history. The film acknowledges the erasure of the Northuldra and their absence from the original film in dramatic terms.</p>\n",
      "      <p>After finding out that their culture would play an even more significant role in the Frozen sequel, the Sámi parliaments of Norway, Sweden and Finland, along with the Saami Council (a non-governmental organization of the Sámi people), reached out to the movie’s producers to collaborate. The filmmakers were on board, and the Sámi groups rounded up a group of experts (called Verddet) to act as cultural consultants for the animation team.</p>\n",
      "      <p>Among the group was Anne Lájla Utsi, managing director at the International Sámi Film Institute, who shared with NOW the ceremonial non-confidential version of a contract drawn up between Walt Disney Animation Studios and the Sámi people.</p>\n",
      "      <p>The contract, signed by Del Vecho and the Sámi parliament reps, is printed to look like a hand-written document. It outlines the studio's “desire to collaborate with the Sámi in an effort to ensure that the content of Frozen 2 is culturally sensitive, appropriate and respectful of the Sámi and their culture.”</p>\n",
      "      <figure>\n",
      "        <img src=\"https://nowtoronto.com/downloads/103112/download/WDASSamiAgreement.png?cb=79075a6bf72a03044aa1889f6b8d8ad8\"/>\n",
      "        <figcaption>\n",
      "          <cite>Courtesy International Sámi Film Institute</cite>\n",
      "        </figcaption>\n",
      "      </figure>\n",
      "      <p>In exchange for the Sámi people’s input, Disney agreed it would also produce a dubbed version of Frozen II in one Sámi language (similar to the <a href=\"https://nowtoronto.com/movies/features/chelsea-winstanley-imaginenative-forgive-me/\">Maori-language, Tahitian and Hawaiian dubs of Moana</a>) and participate in cross-learning initiatives that contribute to Indigenous communities in Scandinavia.</p>\n",
      "      <p>“We have been approached by many outside filmmakers who are interested in Sámi stories,” says Utsi, who is appreciative of Disney’s collaboration and hopes others follow the studio's lead when it comes to telling Sámi stories.</p>\n",
      "      <p>“This is a good example of how a big, international company like Disney acknowledges the fact that we own our own culture and stories. It hasn’t happened before.”</p>\n",
      "      <p>Jesse Wente, director of Canada's Indigenous Screen Office, says he has never seen an agreement like this before, but is enthusiastic about its possibilities.</p>\n",
      "      <p>\"It’s a treaty,\" says Wente. \"It's in keeping with how Indigenous nations have tended to negotiate with other entities in the past. I think it’s a great precedent for how Indigenous nations might deal with a corporation the size of Walt Disney, as well as governments and other agencies, around the use of their cultural and intellectual property in popular entertainment.</p>\n",
      "      <p>\"This is an optimistic moment to show us what could potentially be done here in Canada with a similar studio,\" he adds.</p>\n",
      "      <p>In Frozen II, Fjellheim's Vuelie gets an encore recital, but this time it is sung by the Northuldra characters, a nod to their Sámi-inspired heritage. Other elements in Frozen II inspired by Sámi culture include spirits that represent earth, wind and fire (the Sámi connection to the land is not unlike Indigenous communities in Canada).</p>\n",
      "      <p>There’s also the Northuldra dress, which Utsi explains was a sensitive area. Indigenous communities across the globe have to be wary of how their traditional garments are used considering how they are often appropriated for mascots or Halloween costumes.</p>\n",
      "      <p>“We felt good about them,” says Utsi about the white fur garments worn by Northuldra characters, as well as their traditional use of reindeer and guksi cups.</p>\n",
      "      <p>Disney's efforts to be culturally sensitive in Frozen II come as the studio makes its vault available on the streaming service Disney+. Several old titles like The Jungle Book and Peter Pan have disclaimers warning viewers of \"outdated cultural depictions.\" (The 1946 live action/hybrid <a href=\"https://nowtoronto.com/movies/features/blackface-hollywood-history-cheryl-thompson/\">Song Of The South</a> is not on Disney+.)</p>\n",
      "      <p>The warnings show the company is willing to acknowledge its legacy of racist stereotypes, but advocacy groups believe the disclaimer <a href=\"https://www.hollywoodreporter.com/news/disney-outdated-cultural-depictions-disclaimer-raises-questions-say-advocacy-groups-1255284\">should be extended to more films</a>, including 1992's Aladdin, in which a lyric makes Middle-Eastern characters sound savage.</p>\n",
      "      <p>“We have to push corporations to change, including the ones that have a problematic history,\" says Wente, who cites Disney's 1995 animated film Pocahontas as a movie that has caused harm.</p>\n",
      "      <p>Disney's collaboration with the Sámi helps ensure they won't have to add a disclaimer to Frozen II.</p>\n",
      "      <p>“Disney’s team really wanted to make it right,\" says Utsi. \"They didn’t want to make any mistakes or hurt anybody. We felt that they took it seriously. And the film shows that. We in Verddet are truly proud of this collaboration.”</p>\n",
      "      <figure>\n",
      "        <img src=\"https://nowtoronto.com/downloads/103101/download/FrozenSami2.jpg?cb=1ae7e7a14285e9823af1f10b33d955b8\"/>\n",
      "        <figcaption>Anne Lajla Utsi (centre) among a gathering of Sámi leaders and the Verddet group during the signing of the agreement between Walt Disney Animation Studios and the Sámi people in Oslo.<cite>Courtesy International Sámi Film Institute</cite></figcaption>\n",
      "      </figure>\n",
      "      <p>\n",
      "        <a href=\"https://twitter.com/JustSayRad\">@justsayrad</a>\n",
      "      </p>\n",
      "    </article>\n",
      "  </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "with open(paths[np.random.choice(range(len(paths)))]) as fl:\n",
    "    print(fl.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Society', 25169),\n",
       " ('Entertainment', 18662),\n",
       " ('Economy', 6747),\n",
       " ('Sports', 6638),\n",
       " ('Science', 2394),\n",
       " ('Technology', 2159)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(chain(*table_ru[\"class\"].dropna())).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Entertainment', 36445),\n",
       " ('Sports', 20183),\n",
       " ('Economy', 9490),\n",
       " ('Society', 8379),\n",
       " ('Science', 2132),\n",
       " ('Technology', 1219)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(chain(*table_en[\"class\"].dropna())).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ru**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ru = table_ru.join(pd.read_csv(os.path.join(load_path, \"ru_preproc.csv\"), keep_default_na=False).set_index(\"path\"), on=\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "udpipe_model = Model.load('/eee/tgnews/misc/ru.udpipe')\n",
    "conllu_tokenizer = udpipe_model.newTokenizer(\"conllu\").newConlluInputFormat()\n",
    "\n",
    "def conllu_encode(conllu_text):\n",
    "    \n",
    "    sentence = Sentence()\n",
    "    error = ProcessingError()\n",
    "    \n",
    "    conllu_tokenizer.setText(conllu_text)\n",
    "    \n",
    "    sentences = []\n",
    "    while conllu_tokenizer.nextSentence(sentence, error):\n",
    "        sentences.append(sentence.words[1:]) \n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ru[\"og:title\"] = table_ru[\"og:title\"].apply(conllu_encode)\n",
    "table_ru[\"text\"] = table_ru[\"text\"].apply(conllu_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vowpal_wabbit_encode(sents):\n",
    "    \n",
    "    upostags = {'ADJ', 'INTJ', 'NOUN', 'PROPN', 'VERB'}\n",
    "    \n",
    "    tokens = [[token.lemma for token in sent if token.upostag in upostags] for sent in sents]\n",
    "    tokens = [token.replace(\":\", \"\") for token in chain(*tokens)]\n",
    "    tokens = Counter(tokens)\n",
    "    tokens = [token + ('' if count == 1 else f':{count}') for token, count in tokens.items()]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ru[\"og:title\"] = table_ru[\"og:title\"].apply(vowpal_wabbit_encode)\n",
    "table_ru[\"text\"] = table_ru[\"text\"].apply(vowpal_wabbit_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../table_ru_vw.txt\", \"w\") as fl:\n",
    "    fl.write('\\n'.join(table_ru.apply(lambda x: x[\"path\"] + \" |title_tokens \" + x[\"og:title\"] + \" |text_tokens \" + x[\"text\"], axis=1)) + '\\n')\n",
    "    \n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='../table_ru_vw.txt', \n",
    "                                        data_format='vowpal_wabbit',\n",
    "                                        target_folder='../batches_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vectorizer.dictionary.filter(min_df=3, min_tf=5)\n",
    "\n",
    "batch_vectorizer.dictionary.save_text(dictionary_path='../models/dictionary_ru.txt')\n",
    "batch_vectorizer.dictionary.save(dictionary_path='../models/dictionary_ru.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8\n",
    "num_tokens = 10\n",
    "\n",
    "class_ids = [\"title_tokens\", \"text_tokens\"]\n",
    "topic_names=[\"Society\", \"Economy\", \"Sports\", \"Technology\", \"Entertainment\", \"Science\", \"Other\", \"background\"]\n",
    "\n",
    "docs_for_sst = table_ru[~table_ru[\"class\"].isna()][[\"path\", \"class\"]]\n",
    "n_docs = len(docs_for_sst)\n",
    "\n",
    "doc_titles = docs_for_sst.path.tolist()\n",
    "topic_indecies = docs_for_sst[\"class\"].apply(lambda x: [topic_names.index(topic) for topic in x]).tolist()\n",
    "\n",
    "doc_topic_coef = np.zeros((n_docs, num_topics))\n",
    "for index, topic_index in enumerate(topic_indecies):\n",
    "    doc_topic_coef[index, topic_index] = 1\n",
    "    \n",
    "doc_topic_coef = doc_topic_coef / doc_topic_coef.sum(axis=1)[:, np.newaxis]\n",
    "doc_topic_coef = doc_topic_coef.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = artm.ARTM(num_topics=num_topics, \n",
    "                  class_ids=class_ids,\n",
    "                  topic_names=topic_names,\n",
    "                  dictionary=batch_vectorizer.dictionary, \n",
    "                  theta_columns_naming=\"title\")\n",
    "\n",
    "model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='SST', tau=100, \n",
    "                                                         doc_titles=doc_titles, \n",
    "                                                         doc_topic_coef=doc_topic_coef))\n",
    "\n",
    "model.regularizers.add(artm.SmoothSparsePhiRegularizer('SmoothPhi_back', tau=1, gamma=0, \n",
    "                                                       topic_names=[\"background\"]))\n",
    "\n",
    "model.regularizers.add(artm.DecorrelatorPhiRegularizer('DecPhi', \n",
    "                                                       tau=0.1, \n",
    "                                                       gamma=0))\n",
    "\n",
    "for class_id in class_ids:\n",
    "    model.scores.add(artm.TopTokensScore(name=f'TT_{class_id}', class_id=class_id, num_tokens=num_tokens))\n",
    "    \n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TT_title_tokens\n",
      "Society\n",
      "Россия; Украина; область; дело; задержать; человек; район; погибнуть; суд; житель\n",
      "Economy\n",
      "год; рубль; миллион; Украина; миллиард; газ; банк; мочь; бюджет; рынок\n",
      "Sports\n",
      "матч; сборная; Кубок; победа; тренер; Динамо; чемпионат; команда; игрок; выиграть\n",
      "Technology\n",
      "США; Трамп; выборы; представить; президент; поезд; автомобиль; Зеленский; Турция; Лукашенко\n",
      "Entertainment\n",
      "новый; год; стать; ноябрь; день; пройти; рассказать; фильм; конкурс; Москва\n",
      "Science\n",
      "ученый; врач; назвать; Сирия; рак; продукт; здоровье; опасный; обнаружить; рассказывать\n",
      "Other\n",
      "школа; знак; приложение; школьник; Госдума; Гугл; пользователь; Мелитополь; умный; мусор\n",
      "background\n",
      "к; ро; ча; примета; м; н; ний; р; в; ние\n",
      "\n",
      "TT_text_tokens\n",
      "Society\n",
      "ноябрь; Россия; сообщать; дело; человек; Украина; сообщить; область; время; район\n",
      "Economy\n",
      "рубль; компания; миллион; миллиард; Россия; банк; проект; тысяча; рынок; цена\n",
      "Sports\n",
      "команда; матч; сборная; клуб; чемпионат; игра; победа; тренер; сезон; игрок\n",
      "Technology\n",
      "США; президент; Трамп; выборы; автомобиль; американский; страна; машина; Зеленский; новый\n",
      "Entertainment\n",
      "год; человек; время; быть; стать; день; новый; ноябрь; первый; Россия\n",
      "Science\n",
      "врач; ученый; исследование; заболевание; медицинский; продукт; здоровье; специалист; пациент; организм\n",
      "Other\n",
      "нужный; школа; приложение; устройство; пользователь; родитель; правило; закон; использовать; вода\n",
      "background\n",
      "і; за; уть; як; буть; про; І; який; ал; є\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for modality, modality_top_tokens_tracker in model.score_tracker.items():\n",
    "    print(modality)\n",
    "    for topic_name, modality_top_tokens in modality_top_tokens_tracker.last_tokens.items():\n",
    "        print(topic_name)\n",
    "        print('; '.join(modality_top_tokens))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dump_artm_model('../models/cl_news_model_ru.dump_tm')\n",
    "model.save('../models/cl_news_model_ru.save_tm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**en**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_en = table_en.join(pd.read_csv(os.path.join(load_path, \"en_preproc.csv\"), keep_default_na=False).set_index(\"path\"),  on=\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_en[\"og:title\"] = table_en[\"og:title\"].apply(conllu_encode)\n",
    "table_en[\"text\"] = table_en[\"text\"].apply(conllu_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_en[\"og:title\"] = table_en[\"og:title\"].apply(vowpal_wabbit_encode)\n",
    "table_en[\"text\"] = table_en[\"text\"].apply(vowpal_wabbit_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../table_en_vw.txt\", \"w\") as fl:\n",
    "    fl.write('\\n'.join(table_en.apply(lambda x: x[\"path\"] + \" |title_tokens \" + x[\"og:title\"] + \" |text_tokens \" + x[\"text\"], axis=1)) + '\\n')\n",
    "    \n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='../table_en_vw.txt', \n",
    "                                        data_format='vowpal_wabbit',\n",
    "                                        target_folder='../batches_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vectorizer.dictionary.filter(min_df=3, min_tf=5)\n",
    "\n",
    "batch_vectorizer.dictionary.save_text(dictionary_path='../models/dictionary_en.txt')\n",
    "batch_vectorizer.dictionary.save(dictionary_path='../models/dictionary_en.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8\n",
    "num_tokens = 10\n",
    "\n",
    "class_ids = [\"title_tokens\", \"text_tokens\"]\n",
    "topic_names=[\"Society\", \"Economy\", \"Sports\", \"Technology\", \"Entertainment\", \"Science\", \"Other\", \"background\"]\n",
    "\n",
    "docs_for_sst = table_en[~table_en[\"class\"].isna()][[\"path\", \"class\"]]\n",
    "n_docs = len(docs_for_sst)\n",
    "\n",
    "doc_titles = docs_for_sst.path.tolist()\n",
    "topic_indecies = docs_for_sst[\"class\"].apply(lambda x: [topic_names.index(topic) for topic in x]).tolist()\n",
    "\n",
    "doc_topic_coef = np.zeros((n_docs, num_topics))\n",
    "for index, topic_index in enumerate(topic_indecies):\n",
    "    doc_topic_coef[index, topic_index] = 1\n",
    "    \n",
    "doc_topic_coef = doc_topic_coef / doc_topic_coef.sum(axis=1)[:, np.newaxis]\n",
    "doc_topic_coef = doc_topic_coef.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = artm.ARTM(num_topics=num_topics, \n",
    "                  class_ids=class_ids,\n",
    "                  topic_names=topic_names,\n",
    "                  dictionary=batch_vectorizer.dictionary, \n",
    "                  theta_columns_naming=\"title\")\n",
    "\n",
    "model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='SST', tau=100, \n",
    "                                                         doc_titles=doc_titles, \n",
    "                                                         doc_topic_coef=doc_topic_coef))\n",
    "\n",
    "model.regularizers.add(artm.SmoothSparsePhiRegularizer('SmoothPhi_back', tau=1, gamma=0, \n",
    "                                                       topic_names=[\"background\"]))\n",
    "\n",
    "model.regularizers.add(artm.DecorrelatorPhiRegularizer('DecPhi', \n",
    "                                                       tau=0.1, \n",
    "                                                       gamma=0))\n",
    "\n",
    "for class_id in class_ids:\n",
    "    model.scores.add(artm.TopTokensScore(name=f'TT_{class_id}', class_id=class_id, num_tokens=num_tokens))\n",
    "    \n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TT_title_tokens\n",
      "Society\n",
      "election; trump; court; impeachment; president; house; Ukraine; vote; Impeachment; Johnson\n",
      "Economy\n",
      "trade; China; US; market; bank; price; business; deal; company; Q\n",
      "Sports\n",
      "win; have; game; v; man; star; make; city; team; play\n",
      "Technology\n",
      "Friday; Google; black; get; best; New; deal; Christmas; Pro; Disney\n",
      "Entertainment\n",
      "say; new; year; have; make; US; get; day; protest; first\n",
      "Science\n",
      "man; death; child; health; study; drug; attack; hospital; find; murder\n",
      "Other\n",
      "school; star; crash; student; woman; police; shoot; dog; dead; celebrity\n",
      "background\n",
      "IndiaGlitz.co; sums; Burna; Lottery; Lanez; Winners; Telugu; Davido; en; Nomination\n",
      "\n",
      "TT_text_tokens\n",
      "Society\n",
      "president; party; state; trump; mister; election; government; house; politic; court\n",
      "Economy\n",
      "company; market; year; business; cent; trade; price; bank; growth; stock\n",
      "Sports\n",
      "have; game; go; play; make; year; team; first; time; get\n",
      "Technology\n",
      "new; make; feature; re; Google; black; use; device; get; look\n",
      "Entertainment\n",
      "say; year; have; make; take; be; people; time; go; new\n",
      "Science\n",
      "child; health; study; man; patient; human; drug; care; hospital; find\n",
      "Other\n",
      "school; police; story; life; student; people; film; m; friend; character\n",
      "background\n",
      "ft; க; Album; Orchestra; Nil; Nigeria; songwriter; Tanzania; ம; த\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for modality, modality_top_tokens_tracker in model.score_tracker.items():\n",
    "    print(modality)\n",
    "    for topic_name, modality_top_tokens in modality_top_tokens_tracker.last_tokens.items():\n",
    "        print(topic_name)\n",
    "        print('; '.join(modality_top_tokens))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dump_artm_model('../models/cl_news_model_en.dump_tm')\n",
    "model.save('../models/cl_news_model_en.save_tm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
